# AI Meeting Notes Generator

## Overview
Efficient meeting documentation reduces information loss and improves team accountability.
This project implements an end-to-end audio processing pipeline that records speech, transcribes it using OpenAI Whisper, and generates professional meeting summaries using a local LLM via Docker Model Runner.

The project emphasizes **local-first AI processing**, with no data sent to external cloud services.

---

## Tech Stack
| Component       | Technology                              |
|----------------|-----------------------------------------|
| UI             | Streamlit + audio-recorder-streamlit     |
| Transcription  | OpenAI Whisper (local)                   |
| Summarization  | Llama 3.1 via Docker Model Runner        |
| Data Models    | Pydantic v2                              |
| Database       | SQLite                                   |
| PDF Export     | fpdf2                                    |
| Audio Decoding | FFmpeg                                   |

---

## Project Structure
```
ai-meeting-notes/
├── src/
│   └── meeting_assistant/
│       ├── __init__.py        # Package exports & version
│       ├── __main__.py        # Entry point (python -m)
│       ├── app.py             # Streamlit UI
│       ├── database.py        # Pydantic models & SQLite storage
│       ├── transcriber.py     # Whisper transcription
│       ├── summarizer.py      # LLM summarization
│       └── exporter.py        # PDF export
├── tests/
│   └── test_database.py       # Unit tests
├── uploads/                   # Recorded/uploaded audio
├── exports/                   # Exported PDFs
├── pyproject.toml             # Package configuration
├── .env.example               # Environment config template
├── .gitignore
└── README.md
```

---

## Methodology

### 1. Audio Input
Two input methods are supported:
- **Browser Recording** — Uses `audio-recorder-streamlit` to capture audio directly in the browser, avoiding Streamlit's rerun limitations with backend recording
- **File Upload** — Accepts pre-recorded audio files in WAV, MP3, M4A, OGG, and FLAC formats

Recorded and uploaded files are saved to the `uploads/` directory for processing.

---

### 2. Speech-to-Text Transcription
Audio is transcribed locally using OpenAI's Whisper model:
- FFmpeg decodes audio files into raw waveform data
- Whisper converts speech to text with timestamped segments
- The `base` model (74M parameters) is used by default, balancing accuracy and speed
- FP32 precision is used for CPU compatibility

Whisper supports multiple model sizes (`tiny`, `base`, `small`, `medium`, `large`) for trading off speed against accuracy.

---

### 3. LLM-Powered Summarization
The transcript is processed by Llama 3.1 (8B) running locally via Docker Model Runner. The OpenAI Python SDK is used with a local endpoint, following the OpenAI-compatible API pattern.

Four separate LLM calls are made using **task decomposition**:
- **Title generation** — Short descriptive title (max 10 words)
- **Summary generation** — Professional meeting summary in past tense
- **Key point extraction** — Returns a JSON array of discussion points
- **Action item extraction** — Returns a JSON array of tasks with assignees

Each call uses a focused system prompt with `temperature=0.3` for deterministic output. JSON parsing includes fallback logic for cases where the LLM returns non-JSON responses.

---

### 4. Data Persistence
Meeting records are stored in SQLite using Pydantic v2 models for data validation:
- Lists (key points, action items) are serialized as JSON strings
- An upsert pattern handles both new and updated meetings
- Parameterized queries are used to prevent SQL injection
- The database initializes automatically on first run

---

### 5. PDF Export
Meeting notes can be exported as formatted PDF documents using fpdf2. The export includes title, date, summary, key points (bulleted), action items (bulleted), and the full transcript.

---

## Features

- **Audio Recording** — Record meetings directly in the browser
- **File Upload** — Upload pre-recorded audio files
- **Transcription** — Speech-to-text using OpenAI Whisper
- **AI Summaries** — Professional meeting summaries generated by a local LLM
- **Key Points** — Automatic extraction of important discussion points
- **Action Items** — Identifies tasks and follow-ups with assignees
- **Q&A** — Ask questions about your meetings using the LLM
- **PDF Export** — Export formatted meeting notes
- **Meeting History** — SQLite-backed persistent storage

---

## Design Decisions

### Local-First Architecture
All processing runs on the user's machine. Whisper performs transcription locally, and Llama 3.1 runs via Docker Model Runner. No data leaves the local environment.

### Browser-Based Recording
Initial implementation used `sounddevice` for backend audio capture, but Streamlit's script rerun model caused recorded audio to be lost between interactions. Switching to `audio-recorder-streamlit` resolved this by handling recording entirely in the browser.

### OpenAI SDK for Local Models
Docker Model Runner exposes an OpenAI-compatible API. Using the standard `openai` Python client with a modified `base_url` enables easy switching between local and cloud LLM providers without code changes.

### Task Decomposition over Single Prompt
Rather than asking the LLM to generate all outputs in one call, the pipeline makes four focused calls. This produces more consistent results and allows independent error handling for each output type.

---

## Prerequisites

- Python 3.10+
- [uv](https://docs.astral.sh/uv/) (recommended) or pip
- [FFmpeg](https://ffmpeg.org/download.html) (required by Whisper)
  - Windows: `winget install ffmpeg`
  - Mac: `brew install ffmpeg`
  - Linux: `sudo apt install ffmpeg`
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) with Model Runner enabled:
  ```bash
  docker model pull ai/llama3.1
  ```
- CUDA-capable GPU (recommended for faster Whisper transcription)

---

## How to Run

### 1. Install Dependencies
```bash
git clone https://github.com/yourusername/ai-meeting-notes.git
cd ai-meeting-notes
uv venv
uv pip install -e .
```

### 2. Start the Application
```bash
meeting-assistant
```

Alternative methods:
```bash
python -m meeting_assistant
streamlit run src/meeting_assistant/app.py
```

### 3. Run Tests
```bash
uv pip install -e ".[dev]"
pytest
```

---

## Configuration

Copy `.env.example` to `.env` to customize:

```env
LLM_BASE_URL=http://localhost:12434/engines/llama.cpp/v1
LLM_MODEL=ai/llama3.1
WHISPER_MODEL_SIZE=base
```

---

## License

MIT
